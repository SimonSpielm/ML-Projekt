{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe-F-FilmDialoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allgemeines\n",
    "\n",
    "Eine allgemeine Beschreibung der Laboraufgaben inklusive des Vorgehens, den Bewertungsrichtlinien und der Abgabe finden Sie  <a href=\"ML-allgemein.ipynb\">hier</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenquelle\n",
    "\n",
    "\n",
    "* Laden Sie ihre Daten von http://141.72.190.207/ml_lab/F_dialoge herunter\n",
    "    * Die Daten sind geschützt. \n",
    "        * Sie müssen evtl. in einem Netzwerk der DHBW (z.B. WLAN, VPN, ...) angemeldet sein. \n",
    "        * Sie können sich auf der Webseite mit dem Benutzernamen dhbw und dem Zugangsnamen \"ml_LaB_4$\" anmelden. \n",
    "* Die Daten sind in einem anwendungsspezifischen Format gespeichert.\n",
    "    * Sie finden evtl. Informationen über die Daten in einer \"README\" Datei. \n",
    "    * Finden Sie keine solche Datei sind die Daten selbst erklärend. \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten-Sammlung \n",
    "* besteht aus Dialogen aus verschiedensten Filmen\n",
    "* ist in der Readme Datei beschrieben\n",
    "\n",
    "Erstellen Sie aus den einen Chatbot, der auf eine Frage mit einer Antwort im \"Filmjargon\" antwortet! \n",
    "* Verwenden Sie tiefe Neuronale Netze zu Erstellen des Chatbots! \n",
    "* Passen Sie den Chatbot so an, dass er für unterschiedliche Film-Genres unterschiedlich antwortet! \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lösung\n",
    "\n",
    "Die Lösung der Aufgabe besteht aus mehreren Teilschritten, welche im Folgenden kurz genannt werden:\n",
    "\n",
    "* Daten einlesen\n",
    "* Daten vorverarbeiten\n",
    "* Model erstellen\n",
    "* Model trainieren\n",
    "* Model abspeichern\n",
    "* Model ausführen\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abhängigkeiten des Projekts\n",
    "\n",
    "* Tensorflow 2.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import re\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigene Abhängigkeiten des Projekts\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_helper' from 'g:\\\\My Drive\\\\Dokumente\\\\Studium\\\\Theorie\\\\Vorlesungen\\\\Semester 5\\\\Machine Learning\\\\Projekt\\\\src\\\\data_helper.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "module_path = str(Path.cwd() / \"src\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import data_helper as dh\n",
    "\n",
    "importlib.reload(dh)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen\n",
    "\n",
    "Hier werden zuerst die rohen Daten eingelesen und innerhalb der Hilfsfunktion \"readDataToLines\" mit des \"newline\" Zeichen getrennt. Die Daten werden zwei Listen gespeichert, welche dann zurückgegeben werden.\n",
    "Verwendet wurden folgende Funktionen:\n",
    "\n",
    "* \"open\" um die Datei zu öffnen\n",
    "* \"read\" um die Datei zu lesen\n",
    "* \"split\" um die Daten anhand des \"newline\" Zeichen zu trennen und in einer Liste zu speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you baby\n",
      "fine how are you\n"
     ]
    }
   ],
   "source": [
    "# read the data from the files\n",
    "movie_lines, movie_conversations = dh.readDataToLines(\"data/unzipped/movie_lines.txt\", \"data/unzipped/movie_conversations.txt\")\n",
    "\n",
    "# convert the data into a list of conversations\n",
    "conversations_list = dh.readConversationsToList(movie_conversations)\n",
    "\n",
    "# create a dictionary that maps each line id to the corresponding line\n",
    "id2line = dh.readLinesToDict(movie_lines)\n",
    "\n",
    "# remove all unnecessary characters from the lines and replace short forms with the full words\n",
    "id2line = dh.cleanLines(id2line)\n",
    "\n",
    "# split the conversations into requests and responses, each answer is used as a request for the next answer\n",
    "request, response = dh.splitConversationsToRequestAndResponse(conversations_list, id2line)\n",
    "\n",
    "# delete temporary variables\n",
    "del(movie_lines, movie_conversations, conversations_list, id2line)\n",
    "\n",
    "# optional: limit the size of the dataset\n",
    "request = request[0:len(request)]\n",
    "response = response[0:len(response)]\n",
    "\n",
    "# optional: limit the size of the answers by word count\n",
    "max_wordcount = 100\n",
    "for i in range(len(response)):\n",
    "    # split the response into words\n",
    "    temp = response[i].split()\n",
    "\n",
    "    # limit the size of the list containing one word per entry to the maximum word count\n",
    "    temp = temp[0:max_wordcount]\n",
    "\n",
    "    # convert the list back to a string, whose word count is limited by the maximum word count\n",
    "    response[i] = \" \".join(temp)\n",
    "\n",
    "# optional: limit the size of the requests by word count\n",
    "max_wordcount = 100\n",
    "for i in range(len(request)):\n",
    "    # split the request into words\n",
    "    temp = request[i].split()\n",
    "\n",
    "    # limit the size of the list containing one word per entry to the maximum word count\n",
    "    temp = temp[0:max_wordcount]\n",
    "\n",
    "    # convert the list back to a string, whose word count is limited by the maximum word count\n",
    "    request[i] = \" \".join(temp)\n",
    "\n",
    "del max_wordcount"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten vorverarbeiten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model erstellen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model trainieren"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model abspeichern"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model ausführen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebf27b11610268b99513bd4efbe3432472a3ccaad00eb01d8e50e81655a2bd68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
